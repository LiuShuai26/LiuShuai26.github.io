<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Policy Gradient Method | Shuai’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Policy Gradient Method" />
<meta name="author" content="Shuai Liu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Reinforcement Learning basic concepts" />
<meta property="og:description" content="Reinforcement Learning basic concepts" />
<link rel="canonical" href="http://localhost:4000/rl/2020/12/26/5pg.html" />
<meta property="og:url" content="http://localhost:4000/rl/2020/12/26/5pg.html" />
<meta property="og:site_name" content="Shuai’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-26T01:20:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Policy Gradient Method" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Shuai Liu"},"url":"http://localhost:4000/rl/2020/12/26/5pg.html","headline":"Policy Gradient Method","dateModified":"2020-12-26T01:20:00+08:00","datePublished":"2020-12-26T01:20:00+08:00","description":"Reinforcement Learning basic concepts","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/rl/2020/12/26/5pg.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Shuai's blog" /></head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Shuai&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Policy Gradient Method</h1>
    <p class="post-meta">

      <time datetime="2020-12-26T01:20:00+08:00" itemprop="datePublished">
        
        Dec 26, 2020
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Shuai Liu</span>
      </span>

      <span>
        
      </span>
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/rl/2020/12/26/5pg.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>Reinforcement Learning basic concepts</p>
</blockquote>

<p>Value Based: A policy was generated directly from the value function e.g. using $\epsilon$-greedy</p>

<p>In this lecture we will directly parametrize the policy</p>

<p>Advantages:</p>

<ul>
  <li>Better convergence properties</li>
  <li>Eﬀective in high-dimensional or continuous action spaces</li>
  <li>Can learn stochastic policies</li>
</ul>

<p>Disadvantages:</p>

<ul>
  <li>Typically converge to a local rather than global optimum</li>
  <li>Evaluating a policy is typically ineﬃcient and high variance</li>
</ul>

<hr />

<p>We consider the case of a stochastic, parameterized policy, $\pi_\theta$. Our goal is given policy $\pi_\theta(s,a)$ with parameters $\theta$, ﬁnd best $\theta$. <strong>But how do we measure the quality of a policy $\pi_\theta$?</strong> We use policy objective functions $J(\theta)$. We aim to maximize the expected return $J(\theta)$.</p>

<p>In episodic environments we can use the <strong>start value</strong></p>

\[J_1(\theta) = V^{\pi_\theta}(s_1) = \Bbb E_{\pi_\theta}[v_1]\]

<p>In continuing environments we can use the <strong>average value</strong></p>

\[J_{avV}(\theta) = \underset s \sum d^{\pi_\theta}(s)V^{\pi_\theta}(s)\]

<p>Or the <strong>average reward per time-step</strong></p>

\[J_{avR}(\theta) = \underset s \sum d^{\pi_\theta}(s) \underset a \sum \pi_\theta(s,a)\cal R^a_s\]

<p>where $d^{\pi_\theta}(s)$ is <strong>stationary distribution</strong> of Markov chain for $\pi_\theta$</p>

<hr />

<p>Now on, Policy based reinforcement learning is an <strong>optimization</strong> problem.</p>

<p>Then Find $\theta$ that maximizes $J(\theta)$ using gradient ascend, eg</p>

\[\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}\]

<p>The gradient of policy performance, $\nabla_\theta J(\pi_\theta)$, is called the <strong>policy gradient</strong>, and algorithms that optimize the policy this way are called <strong>policy gradient algorithms.</strong></p>

<hr />

<p>Score Function describes how much better or worse it is than other actions on average (relative to the current policy) in state s.</p>

<p>Assume policy $\pi_\theta$ is differentiable whenever is non-zero and we know the gradient $\nabla_\theta\pi_\theta(s, a)$</p>

<p><strong>Likelihood ratios</strong> exploit the following identity</p>

\[\nabla_\theta\pi_\theta(s, a) = \pi_\theta(s,a)\frac {\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}\\
=\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)\]

<p>The <strong>score function</strong> is $\nabla_\theta\log\pi_\theta(s, a)$</p>

<p>in discrete action spaces, Softmax Policy, score function is</p>

\[\nabla_\theta\log\pi_\theta(s, a) = \phi(s,a)-\Bbb E_{\pi_\theta}[\phi(s, \cdot)]\]

<hr />

<h3 id="policy-gradient-theorem">Policy Gradient Theorem</h3>

<p>The policy gradient theorem generalizes the likelihood ratios approach to multi-step MDPs</p>

<p>$Q^\pi(s,a)$ here is long-term value, could be reward $R_t$.</p>

<blockquote>
  <p>Theorem</p>

  <p>For any differentiable policy $\pi_\theta(s, a)$</p>

  <p>for any of the policy objective functions $J=J_1,J_{avR}, \frac 1 {1-\gamma}J_{avR}$, the policy gradient is 
\(\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}[\nabla_\theta\log{\pi_\theta}(s,a)Q^{\pi_\theta}(s,a)]\)</p>
</blockquote>

<hr />

<h3 id="implementing-the-simplest-policy-gradient">Implementing the Simplest Policy Gradient</h3>

<p>(For full code click <a href="https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py">here</a>.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make core of policy network
</span><span class="n">obs_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">mlp</span><span class="p">(</span><span class="n">obs_ph</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="o">+</span><span class="p">[</span><span class="n">n_acts</span><span class="p">])</span>

<span class="c1"># make action selection op (outputs int actions, sampled from policy)
</span><span class="n">actions</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>This block builds a feedforward neural network categorical policy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make loss function whose gradient, for the right data, is policy gradient
</span><span class="n">weights_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">act_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">action_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">act_ph</span><span class="p">,</span> <span class="n">n_acts</span><span class="p">)</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">action_masks</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">weights_ph</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span>
</code></pre></div></div>

<p>In this block, we build a “loss” function for the policy gradient algorithm. When the right data is plugged in, the gradient of this loss is equal to the policy gradient. The right data means a set of (state, action, weight) tuples collected while acting with environment according to the current policy, where the <strong>weight</strong> for a state-action pair is the reward $R_t$.</p>

<hr />

<h3 id="reducing-variance-using-a-critic">Reducing Variance using a Critic</h3>

<p>Monte-Carlo policy gradient still has high variance.</p>

<p>We use a <strong>critic</strong> to estimate the action-value function</p>

\[Q_w(s, a) \approx Q^{\pi_\theta}(s, a)\]

<p>Actor-critic algorithms maintain two sets of parameters</p>

<p><strong>Critic</strong> Updates action-value function parameters $w$</p>

<p><strong>Actor</strong> Updates policy parameters $\theta$, in direction suggested by critic</p>

<p>Actor-critic algorithms follow an <em>approximate</em> policy gradient</p>

\[\nabla_\theta J(\theta) \approx \Bbb E_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\ Q_w(s,a)] \\
\Delta_\theta = \alpha \nabla_\theta\log \pi_\theta(s,a)\ Q_w(s,a)\]

<hr />

<h3 id="bias-in-actor-critic-algorithms">Bias in Actor-Critic Algorithms</h3>

<p>we can avoid introducing any bias if we choose value function approximation carefully.</p>

<hr />

<h3 id="reducing-variance-using-a-baseline">Reducing Variance using a baseline</h3>

<p>because EGLP lemma, We have $\underset {a_t\sim \pi_\theta} E [\nabla_\theta\log\pi_\theta(a_t|s_t)b(s_t)] = 0$</p>

<p>This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:</p>

\[\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}\left[\nabla_\theta\log{\pi_\theta}(s,a)\left(Q^{\pi_\theta}(s,a)-b(s)\right)\right]\]

<p>A good baseline is the state value function $b(s) = V^{\pi_\theta}(s)$</p>

<p>So we can rewrite the policy gradient using the advantage function $A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)$</p>

<p>describes how much better or worse it is than other actions on average (relative to the current policy).</p>

<hr />

<h3 id="estimating-the-advantage-function">Estimating the Advantage function</h3>

<p>The advantage function can signiﬁcantly reduce variance of policy gradient</p>

<p>So the critic should really estimate the advantage function</p>

<p>For example, by estimating both $V^{\pi_\theta}(s)$ and $Q^{\pi_\theta}(s,a)$</p>

<p>Using two function approximators and two parameter vectors,</p>

\[V_w(s) \approx V^{\pi_\theta}(s)\\
Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\\
A(s,a) = Q_w(s,a) - V_w(s)\]

<p>And updating both value functions by e.g. TD learning</p>

<hr />

<p>Reference:</p>

<p>Reinforcement Learning An Introduction</p>

<p>https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html</p>

<p>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf</p>

  </div>


  <div class="page-navigation">
    
      <a class="prev" href="/rl/2020/11/24/4fa.html">&larr; Function Approximation</a>
    

    
      <a class="next" href="/rl/2020/12/26/6pga.html">Policy Gradient Algorithms &rarr;</a>
    
  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Shuai&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Shuai Liu</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/liushuai26"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">liushuai26</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>