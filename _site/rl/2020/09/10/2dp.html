<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Dynamic Programming | Shuai’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Dynamic Programming" />
<meta name="author" content="Shuai Liu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Reinforcement Learning basic concepts" />
<meta property="og:description" content="Reinforcement Learning basic concepts" />
<link rel="canonical" href="http://localhost:4000/rl/2020/09/10/2dp.html" />
<meta property="og:url" content="http://localhost:4000/rl/2020/09/10/2dp.html" />
<meta property="og:site_name" content="Shuai’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-10T01:20:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Dynamic Programming" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Shuai Liu"},"url":"http://localhost:4000/rl/2020/09/10/2dp.html","headline":"Dynamic Programming","dateModified":"2020-09-10T01:20:00+08:00","datePublished":"2020-09-10T01:20:00+08:00","description":"Reinforcement Learning basic concepts","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/rl/2020/09/10/2dp.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Shuai's blog" /></head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Shuai&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Dynamic Programming</h1>
    <p class="post-meta">

      <time datetime="2020-09-10T01:20:00+08:00" itemprop="datePublished">
        
        Sep 10, 2020
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Shuai Liu</span>
      </span>

      <span>
        
      </span>
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/rl/2020/09/10/2dp.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>Reinforcement Learning basic concepts</p>
</blockquote>

<p>What is dynamic programming?</p>

<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to <strong>compute optimal policies</strong> given a <strong>perfect model of the environment as a Markov decision process (MDP)</strong>.</p>

<p><strong>Dynamic</strong>: sequential or temporal component to the problem</p>

<p><strong>Programming</strong>: optimizing a “program”, i.e. a policy</p>

<p>Dynamic programming assumes full knowledge of the MDP</p>

<p>Dynamic Programming is a very general solution method for problems which have two properties:</p>

<ul>
  <li>Optimal substructure
    <ul>
      <li>Principle of optimality applies</li>
      <li>Optimal solution can be decomposed into subproblems</li>
    </ul>
  </li>
  <li>Overlapping subproblems
    <ul>
      <li>Subproblems recur many times</li>
      <li>Solutions can be cached and reused</li>
    </ul>
  </li>
</ul>

<p>Markov decision processes satisfy both properties</p>

<ul>
  <li>Bellman equation gives recursive decomposition</li>
  <li>Value function stores and reuses solutions</li>
</ul>

<h3 id="policy-evaluation-prediction">Policy Evaluation (Prediction)</h3>

<p>We consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.</p>

\[v_\pi(s) \dot = \underset a \sum \pi(a|s) \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')]\]

<p>where $\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$, and the expectations are subscripted by $\pi$ to indicate that they are conditional on $\pi$ being followed.</p>

\[\begin{align}
v_{k+1}(s)\ &amp; \dot=\  \Bbb E_\pi[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s]\\
&amp; = \underset a \sum \pi(a|s)\underset {s',r}\sum p(s',r|s,a)[r+\gamma v_k(s')]
\end{align}\]

<p>The sequence ${v_k}$ can be shown in general to converge to $v_\pi$ as $k\rightarrow \infty$ under the same conditions that guarantee the existence of $v_\pi$.</p>

<h3 id="policy-improvement">Policy Improvement</h3>

<p>Consider a deterministic policy, $a = \pi(s)$</p>

<p>We can <em>improve</em> the policy by acting greedily</p>

\[\pi'(s) = \underset {a \in A} {\arg \max}\     q_\pi(s, a)\]

<p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that, for all $s\in S$,</p>

\[q_\pi(s, \pi'(s)) \geq v_\pi(s)\]

<p>Then the policy $\pi’$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states  $s\in S$:</p>

\[v_{\pi'} \geq v_\pi(s)\]

<p>new <em>greedy</em> policy, $\pi’$, given by</p>

\[\begin{align}
\pi'(s)\ &amp; \dot =\ \underset a {\arg \max}\ q_{\pi}(s, a)\\
&amp; = \underset a {\arg \max}\ \Bbb E[R_{t+1}+\gamma v_{\pi}(S_{t+1})\ |\ S_t=s, A_t=a]\\
&amp; = \underset a {\arg \max}\ \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')]

\end{align}\]

<h3 id="value-iteration">Value Iteration</h3>

<p>If we know the solution to subproblems $v_<em>(s’)$, then solution $v_</em>(s)$ can be found by one-step lookahead</p>

\[v_*(s) \leftarrow \underset {a\in A} \max R_s^a + \gamma \underset {s'\in S}\sum P_{ss'}^av_*(s')\]

<p>Value iteration is obtained simply by turning the Bellman optimality equation into an update rule.</p>

<p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement.</p>

<h3 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h3>

<p>Three simple ideas for asynchronous dynamic programming:</p>

<ul>
  <li>In-place dynamic programming</li>
  <li>Prioritized sweeping</li>
  <li>Real-time dynamic programming</li>
</ul>

<p><strong>In-place dynamic programming</strong></p>

<p>In-place value iteration only stores one copy of value function. that is, with each new value immediately overwriting the old one. It usually converges faster than the two-copies version, because it uses new data as soon as they are available.</p>

<p><strong>Prioritized Sweeping</strong></p>

<p>Use magnitude of Bellman error to guide state selection. Backup the state with the largest remaining Bellman error. Can be implemented eﬃciently by maintaining a priority queue.</p>

<p><strong>Real-time dynamic programming</strong></p>

<p>Idea: only states that are relevant to agent.</p>

<p>After each time-step agent interactive with environment $S_t, A_t, R_{t+1}$. Only use those experience to Backup:</p>

\[v(S_t) \leftarrow \underset {a\in A} \max \left(R_{S_t}^a + \gamma \underset {s'\in S}\sum P_{S_ts'}^av(s')\right)\]

<h3 id="generalized-policy-iteration">Generalized Policy Iteration</h3>

<p>Almost all reinforcement learning methods are well described as GPI. That is, all have identiﬁable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function.</p>

<h2 id="summary">Summary</h2>

<p>Policy evaluation refers to the (typically) iterative computation of the value functions for a given policy.</p>

<p>Policy improvement refers to the computation of an improved policy given the value function for that policy.</p>

<p>Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods.</p>

<p>It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous DP methods are in-place iterative methods that update states in an arbitrary order.</p>


  </div>


  <div class="page-navigation">
    
      <a class="prev" href="/rl/2020/08/26/1mab.html">&larr; Multi-armed Bandits: exploration and exploitation</a>
    

    
      <a class="next" href="/rl/2020/10/26/3mfp.html">Model-Free Prediction &rarr;</a>
    
  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Shuai&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Shuai Liu</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/liushuai26"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">liushuai26</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>