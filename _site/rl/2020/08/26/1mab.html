<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Multi-armed Bandits: exploration and exploitation | Shuai’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Multi-armed Bandits: exploration and exploitation" />
<meta name="author" content="Shuai Liu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Reinforcement Learning basic concepts" />
<meta property="og:description" content="Reinforcement Learning basic concepts" />
<link rel="canonical" href="http://localhost:4000/rl/2020/08/26/1mab.html" />
<meta property="og:url" content="http://localhost:4000/rl/2020/08/26/1mab.html" />
<meta property="og:site_name" content="Shuai’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-26T01:20:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multi-armed Bandits: exploration and exploitation" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Shuai Liu"},"url":"http://localhost:4000/rl/2020/08/26/1mab.html","headline":"Multi-armed Bandits: exploration and exploitation","dateModified":"2020-08-26T01:20:00+08:00","datePublished":"2020-08-26T01:20:00+08:00","description":"Reinforcement Learning basic concepts","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/rl/2020/08/26/1mab.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Shuai's blog" /></head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Shuai&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Multi-armed Bandits: exploration and exploitation</h1>
    <p class="post-meta">

      <time datetime="2020-08-26T01:20:00+08:00" itemprop="datePublished">
        
        Aug 26, 2020
      </time>

      <span itemprop="author" itemscope itemtype="http://schema.org/Person">
        by <span itemprop="name">Shuai Liu</span>
      </span>

      <span>
        
      </span>
      <!--
      <span class="share-buttons">
        <span class="share-button"><a class="twitter-share-button" href="https://twitter.com/share" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></span>

        <span class="share-button"><span class="fb-like" data-href="/rl/2020/08/26/1mab.html" data-layout="button_count" data-action="like" data-size="small" data-show-faces="false" data-share="true"></span></span>
      </span>
      <div style="clear: both;"/>
      -->

    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <blockquote>
  <p>Reinforcement Learning basic concepts</p>
</blockquote>

<p>You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>

<h3 id="the-multi-armed-bandit">The Multi-Armed Bandit</h3>

<p>A multi-armed bandit is a tuple $\langle \cal A, R\rangle$</p>

<p>$\cal A$ is a known set of $m$ actions (or “arms”)</p>

<p>$\cal R^a(r) = \Bbb P[r|a]$ is an unknown probability distribution over rewards</p>

<p>At each step $t$ the agent selects an action $a_t \in \cal A$</p>

<p>The environment generates a reward $r_t \sim \cal R^{a_t}$</p>

<p>The goal is to maximize cumulative reward $\sum^t_{\tau=1}r_\tau$</p>

<hr />

<p>The action-value is the mean reward for action a</p>

\[q_*(a) \dot= \Bbb E[R_t|A_t=a]\]

<p>We don’t know $q_<em>(a)$. We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_</em>(a)$.</p>

<p>action have the greatest estimated value call greedy action. select greedy action call exploiting.</p>

<p>greedy action selection method as $A_t \dot=\underset a {\arg\max} Q_t(a)$</p>

<p>Greedy action selection always exploits current knowledge to maximize immediate reward.</p>

<p>select one of the nongreedy actions are exploring.</p>

<p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p>

<p>we can’t do exploit and explore at the same time.</p>

<p>A simple alternative is $\epsilon$-greedy methods. which is behave greedily most of the time, but with small probability $\epsilon$ select randomly from among all the actions with equal probability.</p>

<hr />

<h3 id="incremental-implementation">Incremental Implementation</h3>

<p>The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. Incremental implementation can computed these averages in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.</p>

<p>Incremental formulas for updating averages with small, constant computation required to process each new reward.</p>

\[Q_n \dot=\frac {R_1+R_2+ \cdots + R_{n-1}} {n-1}\]

<p>Given $Q_n$ and the $n$th reward, $R_n$, the new average of all n rewards can be computed by</p>

\[Q_{n+1} = \frac 1 n \overset n{\underset {i=1} \sum}R_i \\
= Q_n + \frac 1 n [R_n - Q_n]\]

<h4 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h4>

<p>We often encounter reinforcement learning problems that are effectively nonstationary. that is, the reward probabilities change over time. In such cases it makes sense to give more weight to recent rewards than to long-past rewards.</p>

\[Q_{n+1} \dot= Q_n + \alpha[R_n-Q_n]\]

<p>where the step-size parameter $\alpha \in (0, 1]$ is constant.</p>

<p><strong>Optimistic Initialization</strong>: Simple and practical idea is initialize Q(a) to high value and update action value by incremental Monte-Carlo evaluation. It will encourages systematic exploration early on.</p>

<hr />

<h3 id="upper-conﬁdence-bound-action-selection">Upper-Conﬁdence-Bound Action Selection</h3>

<p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$’s value.</p>

<p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to</p>

\[A_t \dot= \underset a {\arg\max}[Q_t(a)+ c\sqrt{\frac {\ln t} {N_t(a)}}]\]

<p>$N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$.  Each time $a$ is selected the uncertainty is presumably reduced.</p>

<hr />

<h3 id="gradient-bandit">Gradient Bandit</h3>

<p>Here we consider learning a numerical preference for each action $a$, which we denote $H_t(a)$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important</p>

\[Pr\{A_t=a\} \dot= \frac {e^{H_t(a)}} {\sum^k_{b=1}e^{H_t(b)}} \dot=\pi_t(a)\]

<p>On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:</p>

\[H_{t+1}(A_t) \dot=H_t(A_t) + \alpha(R_t-\overset -R_t)(1-\pi_t(A_t)), and\\
H_{t+1}(a) \dot=H_t(a) - \alpha(R_t-\overset -R_t)\pi_t(a), for\ all\ a \not= A_t\]

<p>If the reward is higher than the baseline $\overset - R_t$, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction.</p>

<hr />

<h3 id="summary">Summary</h3>

<p>Exploration and Exploitaion</p>

<ul>
  <li>Exploration ﬁnds more information about the environment</li>
  <li>Exploitation exploits known information to maximize reward</li>
  <li>It is usually important to explore as well as exploit</li>
</ul>

<p>Several simple ways of balancing exploration and exploitation.</p>

<ul>
  <li>The $\epsilon$-greedy methods choose randomly a small fraction of the time,</li>
  <li>UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples.</li>
  <li>Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution.</li>
  <li>The simple expedient of initializing estimates optimistically causes even greedy methods to explore signiﬁcantly.</li>
</ul>

<p>Reference:</p>

<p>Reinforcement Learning An Introduction</p>

<p>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf</p>

  </div>


  <div class="page-navigation">
    
      <a class="prev" href="/rl/2020/07/26/0rlbasic.html">&larr; RL basic conceptions and Finite Markov Decision Processes</a>
    

    
      <a class="next" href="/rl/2020/09/10/2dp.html">Dynamic Programming &rarr;</a>
    
  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Shuai&#39;s blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Shuai Liu</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/liushuai26"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">liushuai26</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>