I"ÿ-<h1 id="rl-basic-conceptions-and-finite-markov-decision-processes">RL basic conceptions and Finite Markov Decision Processes</h1>

<hr />

<h3 id="rewards">Rewards</h3>

<ul>
  <li>A <strong>reward</strong> $R_t$ is a scalar feedback signal</li>
  <li>Indicates how well agent is doing at step $t$</li>
  <li>The agent‚Äôs job is to maximize cumulative reward</li>
</ul>

<p>Reinforcement learning is based on the <strong>reward hypothesis</strong></p>

<blockquote>
  <p><strong>Definition (Reward Hypothesis)</strong></p>

  <p><em>All</em> goals can be described by the maximization of expected cumulative reward.</p>
</blockquote>

<hr />

<h3 id="history-and-state">History and state</h3>

<p>The <strong>history</strong> is the sequence of observations, actions, rewards
<script type="math/tex">H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t</script>
<strong>State</strong> is the information used to determine what happens next.</p>

<p>state is a function of the history:
<script type="math/tex">S_t = f(H_t)</script></p>

<hr />

<h3 id="information-state">Information State</h3>

<p>An <strong>information state</strong> (a.k.a <strong>Markov state</strong>) contains all useful information from the history.</p>

<blockquote>
  <p><strong>Definition</strong></p>

  <p>A state $S_t$ is <strong>Markov</strong> if and only if
<script type="math/tex">\Bbb P[S_{t+1}|S_t] = \Bbb P[S_{t+1}|S_1, ..., S_t]</script></p>
</blockquote>

<ul>
  <li>
    <p>The future is independent of the past given the present
<script type="math/tex">H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infin}</script></p>
  </li>
  <li>
    <p>Once the state is known, the history may be thrown away</p>
  </li>
  <li>
    <p>i.e. The state is a sufficient statistic of the future</p>
  </li>
  <li>
    <p>The environment state $S_t^e$ is Markov</p>
  </li>
  <li>
    <p>The history $H_t$ is Markov</p>
  </li>
</ul>

<hr />

<p><strong>Full observability:</strong> agent directly observes environment state
<script type="math/tex">O_t = S_t^a = S_t^e</script></p>

<ul>
  <li>Agent state = environment state = information state</li>
  <li>Formally, this is a <strong>Markov decision process</strong> (MDP)</li>
</ul>

<p><strong>Partial observability:</strong> agent indirectly observes environment.</p>

<ul>
  <li>Now agent state $\not=$ environment state</li>
  <li>Formally this is a <strong>partially observable Markov decision process</strong> (POMDP)</li>
</ul>

<hr />

<p>An RL agent may include one or more of these components:</p>

<ul>
  <li>Policy: agent‚Äôs behavior function</li>
  <li>Value function: how good is each state and/or action</li>
  <li>Model: agent‚Äôs representation of the environment</li>
</ul>

<p><strong>Policy</strong></p>

<ul>
  <li>A <strong>policy</strong> is the agent‚Äôs behavior</li>
  <li>It is a map from state to action, e.g.</li>
  <li>Deterministic policy: $a = \pi(s)$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Stochastic policy: $\pi(a</td>
          <td>s) = \Bbb P[A_t=a</td>
          <td>S_t=s]$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>Value Function</strong></p>

<ul>
  <li>
    <p>Value function is a prediction of future reward</p>
  </li>
  <li>
    <p>Used to evaluate the goodness/badness of states</p>
  </li>
  <li>
    <p>And therefore to select between actions, e.g.
<script type="math/tex">V_\pi(s) = \Bbb E_\pi[R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + ...|S_t=s]</script></p>
  </li>
</ul>

<p><strong>Model</strong></p>

<ul>
  <li>
    <p>A <strong>model</strong> predicts what the environment will do next</p>
  </li>
  <li>
    <p>$P$ predicts the next state</p>
  </li>
  <li>
    <p>$R$ predicts the next (immediate) reward, e.g.
<script type="math/tex">P_{ss'}^a = \Bbb P[S_{t+1}=s'|S_t=s, A_t=a] \\
R_s^a = \Bbb E[R_{t+1}|S_t=s, A_t = a]</script></p>
  </li>
</ul>

<hr />

<p>Value Based</p>

<ul>
  <li>No Policy (Implicit)</li>
  <li>Value Function</li>
</ul>

<p>Policy Based</p>

<ul>
  <li>Policy</li>
  <li>No Value Function</li>
</ul>

<p>Actor Critic</p>

<ul>
  <li>Policy</li>
  <li>Value Function</li>
</ul>

<p>Model Free</p>

<ul>
  <li>Policy and/or Value Function</li>
  <li>No Model</li>
</ul>

<p>Model Based</p>

<ul>
  <li>Policy and/or Value Function</li>
  <li>Model</li>
</ul>

<hr />

<p>Exploration and Exploitaion</p>

<ul>
  <li>Exploration Ô¨Ånds more information about the environment</li>
  <li>Exploitation exploits known information to maximize reward</li>
  <li>It is usually important to explore as well as exploit</li>
</ul>

<hr />

<hr />

<p><em>Markov decision processes</em> formally describe an environment for reinforcement learning</p>

<p>Where the environment is <em>fully observable</em></p>

<p><strong>Markov Process</strong></p>

<p>A Markov process is a memoryless random process, i.e. a sequence of random states $S_1, S_2, ‚Ä¶$ with the Markov property.</p>

<blockquote>
  <p><strong>Definition</strong></p>

  <p>A <em>Markov Process</em> (or <em>Markov Chain</em>) is a tuple $&lt;S, P&gt;$</p>

  <ul>
    <li>$S$ is a (finite) set of states</li>
    <li>$P$ is a state transition probability matrix,</li>
  </ul>

  <script type="math/tex; mode=display">P_{ss'} = \Bbb P[S_{t+1} = s'|S_t=s]</script>

  <p><strong>Definition</strong></p>

  <p>A <em>Markov Reward Process</em> (or <em>Markov Chain</em>) is a tuple $&lt;S, P, R, \gamma&gt;$</p>

  <ul>
    <li>$S$ is a (finite) set of states</li>
    <li>$P$ is a state transition probability matrix,</li>
  </ul>

  <script type="math/tex; mode=display">P_{ss'} = \Bbb P[S_{t+1} = s'|S_t=s]</script>

  <ul>
    <li>
      <table>
        <tbody>
          <tr>
            <td>$R$ is a reward function, $R_s = \Bbb E[R_{t+1}</td>
            <td>S_t=s]$</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li>$\gamma$ is a discount factor, $\gamma \in [0, 1]$</li>
  </ul>

  <p><strong>Definition</strong></p>

  <p>The <em>return</em> $G_t$ is the total discounted reward from time-step $t$.
<script type="math/tex">G_t = R_{t+1} + \gamma R_{t+2} +\ ...=\underset {k=0} {\overset \infin \sum} \gamma^kR_{t+k+1}</script></p>

  <ul>
    <li>The <em>discount</em> $\gamma \in [0, 1]$ is the present value of future rewards</li>
    <li>The value of receiving reward R after k+1 time-steps is $\gamma^kR$</li>
    <li>This values immediate reward above delayed reward.</li>
    <li>$\gamma$ close to 0 leads to ‚Äúmyopic‚Äù evaluation</li>
    <li>$\gamma$ close to 1 leads to ‚Äúfar-sighted‚Äù evaluation</li>
  </ul>

  <p>The value function $v(s)$ gives the long-term value of state $s$</p>

  <p><strong>Definition</strong></p>

  <p>The <em>state value function</em> v(s) of an MRP is the expected return starting from state $s$
<script type="math/tex">v(s) = \Bbb E[G_t|S_t=s]</script></p>
</blockquote>

<p><strong>Bellman Equation for MRPs</strong></p>

<p>The value function can be decomposed into two parts:</p>

<ul>
  <li>immediate reward $R_{t+1}$</li>
  <li>discounted value of successor state $\gamma v(S_{t+1})$</li>
</ul>

<script type="math/tex; mode=display">v(s) = \Bbb E[G_t|S_t=s] = \Bbb E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]</script>

<hr />

<p><strong>Markov Decision Process</strong></p>

<p>A Markov decision process (MDP) is a Markov reward process with decisions. It is an <em>environment</em> in which all states are Markov.</p>

<blockquote>
  <p><strong>Definition</strong></p>

  <p>A <em>Markov Decision Process</em> is a tuple $&lt;S, A, P, R, \gamma&gt;$</p>

  <ul>
    <li>$S$ is a finite set of states</li>
    <li>A is a finite set of actions</li>
    <li>P is a state transition probability matrix,</li>
  </ul>

  <table>
    <tbody>
      <tr>
        <td>$P_{ss‚Äô}^a=\Bbb P[S_{t+1}=s‚Äô</td>
        <td>S_t=s, A_t=a]$</td>
      </tr>
    </tbody>
  </table>

  <ul>
    <li>
      <table>
        <tbody>
          <tr>
            <td>R is a reward function, $R_s^a = \Bbb E[R_{t+1}</td>
            <td>S_t=s, A_t=a]$</td>
          </tr>
        </tbody>
      </table>
    </li>
    <li>$\gamma$ is a discount factor $\gamma \in [0, 1]$</li>
  </ul>
</blockquote>

<p><strong>Policies</strong></p>

<blockquote>
  <p><strong>Definition</strong></p>

  <p>A <em>policy</em> $\pi$ is a distribution over actions given states,
<script type="math/tex">\pi(a|s) = \Bbb P[A_t=a|S_t=s]</script></p>

  <ul>
    <li>A policy fully defines the behavior of an agent</li>
    <li>MDP policies depend on the current state (not the history)</li>
  </ul>
</blockquote>

<p><strong>Value Function</strong></p>

<blockquote>
  <p><strong>Definition</strong></p>

  <p>The <em>state-value</em> function $v_\pi(s)$ of an MDP is the expected return starting from state s, and then following policy $\pi$
<script type="math/tex">v_\pi(s) = \Bbb E_\pi[G_t|S_t=s]</script>
The <em>action-value</em> function $q_\pi(s, a)$ is the expected return starting from state s, taking action a, and then following policy $\pi$
<script type="math/tex">q_\pi(s, a)=\Bbb E_\pi[G_t|S_t=s, A_t=a]</script></p>
</blockquote>

<hr />

<p>Bellman Expectation Equation</p>

<p>The state-value function can again be decomposed into immediate reward plus discounted value of successor state,
<script type="math/tex">v_\pi(s) = \Bbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]</script>
The action-value function can similarly be decomposed,
<script type="math/tex">q_\pi(s, a)=\Bbb E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1}, A_{t+1})|S_t=s, A_t=a]</script></p>

<hr />

<h3 id="formula-derivation">Formula derivation:</h3>

<p>Expected discounted return:
<script type="math/tex">% <![CDATA[
\begin{align}
G_t & \dot= R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \gamma^3R_{t+4}+ \dots \\
& = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2R_{t+4}+ \dots) \\
& = R_{t+1} + \gamma G_{t+1}
\end{align} %]]></script>
where $\gamma$ is a parameter, $0 \le \gamma \le 1$, called the discount rate.</p>

<p>Bellman equation for $v_\pi$
<script type="math/tex">% <![CDATA[
\begin{align}
v_\pi(s) &\dot=\Bbb E_\pi[G_t|S_t=s]\\
&= \Bbb E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
&= \underset a \sum\pi(a|s)\underset {s'} \sum\underset r \sum p(s', r|s,a)[r+\gamma \Bbb E_\pi[G_{t+1}|S_{t+1}=s']]\\
&=\underset a \sum \pi(a|s) \underset {s',r} \sum p(s', r|s,a)[r+\gamma v_\pi(s')],\ for\ all\ s \in \cal S
\end{align} %]]></script></p>

<hr />

<p>We denote all the <em>optimal policies</em> by $\pi_*$.</p>

<p>$\pi \ge \pi‚Äô$ if and only if $v_\pi(s) \ge v_{\pi‚Äô}(s)$ for all $s \in \cal S$</p>

<p><em>optimal state-value function</em></p>

<p>$v_*(s) \dot= \underset \pi \max v_\pi(s)$, for all $s \in \cal S$</p>

<p><em>optimal action-value function</em>, denoted $q_*$:</p>

<p>$q_*(s, a) \dot=\underset \pi \max q_\pi(s, a)$, for all $s \in \cal S$ and $a \in \cal A(s)$.</p>

<p>$q_<em>$ in terms of $v_</em>$:</p>

<table>
  <tbody>
    <tr>
      <td>$q_<em>(s, a) = \Bbb E[R_{t+1} + \gamma v_</em>(S_{t+1})</td>
      <td>S_t=s, A_t=a]$.</td>
    </tr>
  </tbody>
</table>

<hr />

<p>Bellman optimality equation for $v_<em>$
<script type="math/tex">% <![CDATA[
\begin{align}
v_\pi(s) &= \underset {a \in \cal A(s)} \max q_{\pi_*}(s, a)\\
&= \underset a \max \Bbb E_{\pi_*}[G_t|S_t=s, A_t=a]\\
&= \underset a \max \Bbb E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s, A_t=a]\\
&= \underset a \max \Bbb E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a] \\
&=\underset a \max\underset {s',r} \sum p(s', r|s,a)[r+\gamma v_\pi(s')]
\end{align} %]]></script>
Bellman optimality equation for $q_</em>$
<script type="math/tex">q_*(s, a) = \Bbb E[R_{t+1} + \gamma \underset {a'} \max q_*(S_{t+1}, a') | S_t=s, A_t=a]\\
= \underset {s', r} \sum p(s', r|s, a)[r+ \gamma \underset {a'} \max q_*(s', a')].</script></p>
:ET